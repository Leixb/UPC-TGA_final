%! TEX root = **/010-main.tex
% vim: spell spelllang=es:

% Antes de empezar el examen, leed atentamente los siguientes comentarios:
%
% - Esto es un examen y los exámenes se hacen de forma individual.
%
% - La fecha límite para entregar el examen (y el proyecto) es el lunes 21 de
%   junio de 2021 a las 8:00.
%
% - He montado una “práctica” en el racó para hacer la entrega del examen.
%
% - El examen que entreguéis ha de ser un PDF.
%
% - Hemos calculado que el tiempo necesario para realizar el examen son unas
%   20-30 horas de trabajo. No lo dejéis para el final.
%
% - Teniendo en cuenta que tenéis mucho tiempo para hacer el examen, una buena
% redacción y ortografía se tendrá en cuenta en la evaluación.
%
% - No os limitéis a la información que hay en las transparencias del curso, hay
%   que buscar en otras fuentes.
%
% - Además del examen tenéis que entregar un anexo, en el que, para cada
%   pregunta, indiquéis las referencias, páginas web, etc. que habéis consultado
%   durante el examen.
%
% - El examen consta de:
%   - 6 preguntas estándar (2 hojas por pregunta, aprox.  1000 palabras por pregunta).
%   - 1 pregunta doble, es cómo un pequeño trabajo (4 hojas, aprox. 2000 palabras)

\begin{pregunta}{Describe el pipeline gráfico tradicional}\sep{}

El pipeline gráfico tradicional tiene como objetivo la generación de gráficos
explotando al máximo el paralelismo que ofrecen las GPUs. Para ello el pipeline
organiza las distintas tareas a realizar de forma coherente aprovechando al
máximo las capacidades de las GPUs y ocultando la latencia de memoria. Este
pipeline es relativamente simple en comparación al actual y muy limitado en
capacidad de programar su funcionamiento: solo hay dos \emph{shaders} que pueden
hacer operaciones muy limitadas.

El pipeline se puede dividir en 4 etapas principales. Aunque hay un orden en
estas etapas, algunas operaciones se pueden realizar simultáneamente y así
aprovechar el paralelismo que ofrece la GPU.

\begin{enumerate}
    \item Vértices
    \item Primitivas
    \item Fragmentos
    \item Pixels
\end{enumerate}

\paragraph{Vértices} En el procesado de vértices se proyectan los vértices sobre
el plano de visión según la configuración de proyección y la camera. También se
pueden aplicar cálculos de iluminación y color o otras transformaciones
sobre los vértices del modelo o la cámara. En el pipeline tradicional estas
operaciones se realizan en el \emph{vertex shader} donde se programa como se
hacen los cálculos. Estas operaciones se realizan en paralelo para diversos
grupos de pixels.

\paragraph{Primitivas} Partiendo de los vértices procesados en la etapa anterior
se generan primitivas agrupándolos en triángulos. Se eliminan primitivas
innecesarias (que no se verán en la imagen final). Para ello se aplica un
proceso de \emph{CLIPPING} y \emph{CULLING}. El \emph{CLIPPING} consiste en
eliminar las primitivas que queden fuera del campo de visión (o parte de ellas,
dividiéndolas en triángulos). \emph{CULLING} usa el \emph{z-buffer} para
determinar que primitivas quedan ocultas detrás de otras y eliminar-las de modo
similar al \emph{CLIPPING}. En algunos casos la etapa de \emph{CULLING} no se
puede aplicar debido a la transparencia de los objetos. Eliminar primitivas
permite evitar computaciones innecesarias en las siguientes etapas.

Antes de pasar a la siguiente fase, se calculan las ecuaciones que forman los
triángulos (\emph{triangle setup}) que serán usadas en las otras etapas. Este
calculo es muy costoso y poco eficiente si tenemos triángulos muy pequeños.

\paragraph{Fragmentos} En esta etapa se generan y procesan los fragmentos
(grupos de pixels de la imagen que pertenecen a un mismo triangulo). La
generación de los fragmentos se denomina \emph{rasterización} y consiste en
calcular para cada triangulo que pixels de la imagen final forman parte de el.
Una vez \emph{rasterizados} los fragmentos se procesan en el \emph{fragment
shader} que permite aplicar transformaciones a los fragmentos (como antialiasing) y
calcular su color (u otros atributos). Al igual que el \emph{vertex shader}, se
puede programar. En el \emph{fragment shader} se puede aplicar texturas
almacenadas en la memoria de texturas de la tarjeta.

\paragraph{Pixels} En la etapa final se aplican operaciones sobre los fragmentos
antes de trasladar-los al \emph{frame-buffer}. Hay dos tipos de operaciones
posibles, las de aceptación/rechazo en las que se decide si el pixel se copiara
al \emph{frame buffer} o no y las de \emph{combinación} que combinan los colores
de varios pixels. Las operaciones de aceptación/rechazo se realizan mediante
\emph{tests}. Tenemos los siguientes:

\begin{enumerate}
    \item \textbf{Scissor test} Recorta el área de renderizado. Solo hace falta
        almacenar los parámetros que definen el área rectangular de renderizado.
    \item \textbf{Alpha test} Descarta pixels con valor \emph{alpha}
        (transparencia) de 0 o muy cercanos a 0.
    \item \textbf{Stencil test} Usado para dibujar sombras: se proyectan sobre
        el \emph{stencil buffer} y al renderizar se hacen dos pasadas dibujando
        los elementos fuera del \emph{stencil} con iluminación y los marcados en
        el sin iluminación. También se puede usar para recortar el área de
        renderizado con formas no rectangulares.
    \item \textbf{Z test} Descarta fragmentos que quedan ocluidos por otros.
        Es una operación muy costosa y hay varias optimizaciones que se han ido
        aplicando a lo largo de los años.
    \item \textbf{Alpha Blending} Determina como combinar el color del
        \emph{frame buffer} con el del nuevo pixel según su transparencia (\emph{alpha})
        mediante un proceso de interpolación.
\end{enumerate}

Actualmente las etapas del pipeline gráfico tradicional se siguen aplicando en
las GPUs pero han habido muchos cambios en su funcionamiento y arquitectura.
Quizás el más notable son los \emph{shaders}. En las tarjetas gráficas actuales
los \emph{shaders} estan unificados, que permiten que los \emph{vertex} y
\emph{fragment shaders} usen los mismos elementos de cálculo. También
hay nuevos \emph{shaders} (\emph{Geometry}, \emph{Hull}, \emph{Domain})
que las APIs gráficas pueden implementar ya que al estar unficados los \emph{shaders}
se puede mezclar mejor los elementos programables y los fijos y añadir procesos
programables en el pipeline sin necesidad de cambiar la arquitectura de la tarjeta.
Además los \emph{shaders} tienen más capacidades permitiendo acceso a memoria
convencional instrucciones de control de flujo y más registros.

\paragraph{Referencias:}
\cite{lawrence_3d_2012}
\cite{rmit_-_cs_lecture_2017}
\cite{moller_real-time_2018}
\cite{giesen_trip_2011}

\end{pregunta}

\begin{pregunta}{Dada la siguiente rutina escrita en C:}
    \begin{minted}{c}
    void Examen21(float mA[N][M], float mB[N][M], float vC[N], float vD[M]) {
      int i, j;
      for (i=0; i<N; i++)
        for (j=0; j<M; j++)
             mA[i][j] = mA[i][j]*vC[i] - mB[i][j]*vD[j] + mA[i][0]*mB[7][j];
    }
    \end{minted}

    Escribid 3 versiones del kernel CUDA que resuelva el mismo problema:

\begin{enumerate}[label=(\alph*)]
    \item En la primera versión cada thread se va a ocupar de 1 columna de la matriz resultado.
    \item En la segunda versión cada thread se va a ocupar de 1 fila de la matriz resultado.
    \item En la última versión cada thread se va a ocupar de 1 elemento de la matriz resultado.
\end{enumerate}

Escribid los kernels CUDA para cada versión, así como la invocación
correspondiente. Tened en cuenta que como máximo podéis utilizar 1024 threads
por bloque y que las variables N y M pueden tener cualquier valor (p.e. N =
1237, M = 2311, suponed que N, M > 1024).

\vspace{1em} \sep{}
\vspace{3em}

Si analizamos las dependencias del código, vemos que \texttt{mA[i][j]} depende
del valor \texttt{mA[i][0]}, por lo que se tiene que calcular siempre la columna
0 antes que el resto. En el caso de 1 thread por fila no hay problema, pero los
otros deben sincronizarse adecuadamente.

Otro apunte importante es que trabajar con matrices en CUDA es complejo y puede
generar problemas (alocución no contigua en memoria de hileras), por lo que en
mi versión se usaran matrices aplanadas.

El código completo se encuentra en el~\cref{asec:codigo}.

\pagebreak
\subsection*{(a) Columna}

Para el \emph{kernel} de columna tenemos que asegurar-nos de que la primera columna
se ha calculado antes de proceder al resto. Para ello usamos un \emph{lock} que
se tiene que inicializar a 0 antes de la llamada al \emph{kernel} y al que solo el
primer thread del primer bloque escribe. Usamos un \texttt{while} para esperar a que
el valor del \emph{lock} este en 1 sincronizando los threads.

Otra alternativa sería ejecutar la primera columna en serie antes del \emph{kernel} y que el
thread de la primera columna no hiciera nada.

\begin{listing}[H]
    \caption{Kernel columna y su invocación}
    \inputminted[firstline=17,lastline=33]{cuda}{code/main.cu}
    \vspace{-2em}
    \inputminted[firstline=120,lastline=127,autogobble]{cuda}{code/main.cu}
    \label{lst:kernel-columna}
\end{listing}

\pagebreak
\subsection*{(b) Fila}

Para el \emph{kernel} de fila si empezamos cada fila por la columna 0 no hay ningún
problema de dependencias entre threads.

\begin{listing}[H]
    \caption{Kernel fila y su invocación}
    \inputminted[firstline=35,lastline=41]{cuda}{code/main.cu}
    \vspace{-2em}
    \inputminted[firstline=114,lastline=117,autogobble]{cuda}{code/main.cu}
    \label{lst:kernel-fila}
\end{listing}

\pagebreak
\subsection*{(c) Elemento}

Al igual que en el caso de las columnas, para calcular un elemento es necesario que
el valor de la primera columna de la fila en el que se encuentra el elemento se haya
calculado antes. Pero a diferencia del caso de las columnas, podemos calcular todos los
elementos de la primera columna en paralelo. Para ello usaremos un \emph{lock} de
\emph{N} posiciones en el que el primer bloque del primer thread escribirá en la posición
de la fila en la que se ha calculado. Al igual que en el caso de las columnas usamos un
\texttt{while} con el \emph{lock} para esperar a que se completen estos elementos.

\begin{listing}[H]
    \caption{Kernel elemento y su invocación}
    \inputminted[firstline=43,lastline=57]{cuda}{code/main.cu}
    \vspace{-2em}
    \inputminted[firstline=130,lastline=137,autogobble]{cuda}{code/main.cu}
    \label{lst:kernel-elemento}
\end{listing}

\nocite{noauthor_cuda_nodate}

\end{pregunta}

\begin{pregunta}{Disponemos de una tarjeta gráfica con 2 GPUs. En esta tarjeta queremos
    correr un juego interactivo 3D (que utiliza OpenGL u otra API similar). Si
    estuvierais diseñando el driver de la API gráfica, ¿cómo distribuirías el
    trabajo entre las 2 GPUs para maximizar el rendimiento?  ¿Qué información
    hay que enviar a cada tarjeta? ¿Han de sincronizarse/comunicarse las 2 GPUs?
    ¿Cómo pueden hacerlo? Os ayudará tener en mente cómo funciona el pipeline
gráfico} \sep{}

Hay varias opciones de como dividir el trabajo entre las \emph{GPU}s. Si
consideramos el pipeline gráfico podemos partir el trabajo dividiendo el área a
renderizar en dos secciones, una para cada tarjeta (\emph{Split Frame
Rendering}). Alternativamente podemos hacer que las tarjetas renderizen
\emph{frames} alternados (\emph{Alternate Frame Rendering}) o combinar los
graficos de las dos tarjetas para hacer Antialiasing.

En todos los casos solo una \emph{GPU} mostrara la imagen en pantalla por lo que
toda la información generada por la otra \emph{GPU} se tiene que transferir a la
tarjeta principal a la que esta conectada el monitor. Esto se podría evitar
teóricamente si se usaran dos pantallas conectadas a gráficas distintas pero no
es habitual. \cite{noauthor_sli_2011}

\subsection*{SFR}

En el caso de \emph{SFR} (\emph{Split Frame Rendering}) al separar la imagen
final en dos partes se puede reducir el uso de memoria de las tarjetas.

Para poder separar la imagen en dos partes debemos aplicar el proceso de
\emph{CULLING} usando una parte del área de renderizado en una tarjeta y la otra
mitad en la otra. Para ello la  mejor opción es enviar todos los pixels a las
tarjetas y que hagan \emph{CULLING} con distintos parámetros. Por lo que se
tiene que enviar toda la información de vértices de la escena a las dos gráficas
simultáneamente.

La ventaja de este método es que el uso de recursos de las tarjetas individuales
es menor ya que solo deben almacenar la mitad de información de primitivas, fragmentos y
pixels. Este menor uso de memoria puede permitir renderizar a resoluciones
mayores que no se podrían conseguir con solo una de las gráficas por problemas
de memoria.

La desventaja es que la sincronización ha de ser
perfecta entre las dos \emph{GPU}s y deben funcionar de la misma manera para
garantizar que no se vean cambios aparentes entre las dos partes de la imagen
final. También Pueden surgir problemas de aliasing en la zona en la que se unen
las imágenes.

\subsection*{AFR}

Con \emph{AFR} (\emph{Alternate Frame Rendering}) las dos tarjetas tienen que
tener toda la información de la escena, ya que tienen que computar la escena
entera las dos. Esto implica que se debe enviar toda la información a las dos
tarjetas al igual que en el caso de \emph{SFR}. Sin embargo, las dos tarjetas
deben calcular toda la escena por lo que el uso de memoria en las tarjetas es
el mismo que si se usaran individualmente.

\subsection*{Anti-aliasing}

Esta técnica consiste en calcular varios puntos usados en el proceso de
\emph{anti-aliasing} en las dos \emph{GPU}s usando distintos puntos
de sampling en las dos \emph{GPU}s. Esto permite usar el doble (o mas) de puntos
para calcular el \emph{anti-aliasing} ofreciendo mejor calidad de imagen.

Para aplicar \emph{anti-aliasing} se necesita información de fragmentos y de las
texturas. La memoria de texturas se puede copiar des del \emph{host} pero los
datos de los fragmentos tienen que haber sido calculados por la tarjeta gráfica.
Para ello se deben computar los fragmentos, transferir-los a la segunda tarjeta,
hacer el \emph{sampling} y devolver la información de las \emph{samples} a la
tarjeta principal donde se combinan y se produce la imagen final. Por lo tanto
hay que poder comunicar las tarjetas para poder enviar los fragmentos y recibir
los \emph{samples}.

\subsection*{Tareas no gráficas}

Para todos los métodos descritos antes se tiene que poder copiar toda la
información de la segunda tarjeta a la principal para poder mostrar la imagen
final en pantalla (o realizar el \emph{anti-aliasing}). En la práctica, para
resoluciones actuales usar los buses de \emph{PCI} puede saturar el ancho de
banda ya que no solo se tiene que comunicar una tarjeta con la otra sino que
también se deben copiar los datos de la aplicación.

Para poder realizar estos métodos de forma eficiente se debe usar
una conexión directa entre las tarjetas que permita un gran ancho de banda capaz
de transmitir los \emph{frames} entre tarjetas rápidamente. Las tarjetas
gráficas de \emph{Nvidia} ofrecen esta funcionalidad a través de una conexión
llamada \emph{SLI} que conecta directamente las tarjetas con un cable
\cite{noauthor_sli_nodate}. La misma
tecnología existe en el caso de las tarjetas \emph{AMD} a la que llaman
\emph{Crossfire} \cite{noauthor_amd_nodate}. Actualmente las nuevas tarjetas de
estas empresas ya no ofrecen esta funcionalidad.

Si no se dispone de \emph{SLI} o \emph{Crossfire} se puede usar la segunda
tarjeta para realizar tareas no relacionadas con la renderizaci\'on. Por ejemplo
se podría usar para hacer cálculos del sistema de simulación física del juego.
En el caso de \emph{Nvidia} el propio usuario puede configurar que el sistema
\emph{PhysX} (simulación física de \emph{Nvidia}) use la \emph{GPU} secundaria.



\end{pregunta}
\begin{pregunta}{Una de las herramientas que utilizamos en CUDA son los eventos (event en
CUDA). ¿Para qué sirven? ¿Cómo se utilizan? Pon ejemplos de uso.} \sep{}

Los eventos en \emph{CUDA} sirven para marcar puntos en la ejecución de un \emph{stream}.
\cite{noauthor_event_nodate}. Nos permiten obtener métricas sobre los instantes en los
que se producen ciertas operaciones durante la ejecución del \emph{stream}

Estos eventos de \emph{CUDA} se pueden usar para medir el tiempo de ejecución
de tareas en un \emph{stream}, sincronizar tareas en un mismo \emph{stream} o
entre distintos \emph{streams}.

\subsection*{Funcionamiento}

El funcionamiento de los eventos es muy simple y esta altamente relacionado con
el funcionamiento de los \emph{streams}:

\begin{itemize}
    \item Creamos el evento con: \texttt{cudaEventCreate(event)}

        En este punto solo hemos creado el evento y hemos almacenado su
        referencia en la variable \texttt{event}.

    \item Registramos el evento con: \texttt{cudaEventRecord(event, stream)}

        Con esta funcion registramos el evento en el \emph{stream} especificado
        por la variable \texttt{stream}. Cuando este \emph{stream} acabe de
        ejecutar lo que tenia por delante en la cola almacenara el tiempo en el
        que ha llegado al evento en nuestra variable evento (a nivel de
        \emph{GPU}).

    \item Sincronizamos el evento con: \texttt{cudaEventSynchronize(event)}

        Antes de poder consultar el valor del evento tenemos que sincronizar con
        la \emph{GPU} para asegurar que se ha llegado ya a dar un valor a este.

    \item Calculamos el tiempo entre eventos con: \texttt{cudaEventElapsedTime(\&t, start, end)}

        Esta función calcula el tiempo entre dos eventos y los almacena en la
        variable \texttt{t} (de tipo \texttt{float}).

    \item También podemos consultar si se ha llegado al evento con:
        \texttt{cudaEventQuery(event)}

    \item Eliminamos el evento con: \texttt{cudaEventDestroy(event)}

    \item Para la sincronización entre \emph{streams} podemos usar:
        \texttt{cudaStreamWaitEvent(stream, event, flags)}

        Esta función registra en el \emph{stream} especificado un punto del que
        no se puede pasar hasta que no termine el evento.
\end{itemize}

\subsection*{Ejemplos}

En el~\cref{lst:kernel-event} podemos ver un ejemplo de como usar eventos para
medir el tiempo de ejecución (en milisegundos). En este caso solo hace falta sincronizar el
último evento ya que si se ha llegado a ese, el resto también.

\begin{listing}[H]
    \caption{Medir el tiempo de ejecución de un kernel con \emph{events}}
    \label{lst:kernel-event}
    \begin{minted}[autogobble]{cuda}
    cudaEvent_t start, end;
    cudaEventCreate(&start); cudaEventCreate(&end);

    cudaEventRecord(start, 0);
    kernel<<<256,256>>>(...);
    cudaEventRecord(end, 0);

    float tiempoKernel;
    cudaEventSynchronize(end);
    cudaEventElapsedTime(&tiempoKernel, start, end);

    cudaEventDestroy(start); cudaEventDestroy(end);

    printf("Tiempo de ejecución: %dms\n", tiempoKernel)
    \end{minted}
\end{listing}

En el~\cref{lst:kernel-event-streams} podemos ver un ejemplo del uso de eventos
para la sincronización entre dos \emph{streams} distintos. En este caso el
kernel $A$ se ejecuta en el \emph{stream} 1 y los kernels $B$ y $C$ en el
\emph{stream} 2. El evento \texttt{lock} se registra en el \emph{stream} 1
cuando el kernel $A$ ha terminado. En el \emph{stream} 2, se espera a que este
evento se acabe antes de continuar y ejecutar el kernel $C$.

De este modo los kernels $A$ y $B$ se pueden ejecutar concurrentemente y $C$
solo se puede ejecutar cuando $A$ acabe (y $B$ también ya que están en el mismo
\emph{stream}). Se podría conseguir el mismo resultado usando una llamada a
\texttt{cudaDeviceSynchronize()} antes de llamar al kernel $C$.

\begin{listing}[H]
    \caption{Sincronización de \emph{streams} con \emph{events}}
    \label{lst:kernel-event-streams}
    \begin{minted}[autogobble]{cuda}
    cudaEvent_t lock;
    cudaEventCreate(&lock);
    cudaStream_t stream1, stream2;
    cudaStreamCreate (&stream1); cudaStreamCreate (&stream2);

    kernel_A<<<256, 256, stream1>>>(...);
    cudaEventRecord(lock, stream1);

    kernel_B<<<256, 256, stream2>>>(...);
    cudaStreamWaitEvent(stream2, lock, 0);
    kernel_C<<<256, 256, stream2>>>(...);

    cudaEventDestroy(lock);
    \end{minted}
\end{listing}

\end{pregunta}
\begin{pregunta}{Si queremos utilizar GPUs para cálculo de propósito general (GPGPU)
    puedes escoger entre CUDA, OpenCL y OpenACC. Describe las ventajas e
    inconvenientes de cada alternativa.  Además, se pueden combinar. ¿Qué
posibilidades ofrece combinar OpenACC con CUDA o OpenCL?} \sep{}

\paragraph{CUDA} es la solución de \emph{GPGPU} ofrecida por \emph{Nvidia}.
Es un estándar propietario y cerrado controlado por \emph{Nvidia} y que solo se
puede usar con sus tarjetas gráficas. \cite{noauthor_cuda_2017}

La gran ventaja de \emph{CUDA} es el soporte que ofrece \emph{Nvidia} para
integrar aplicaciones con \emph{CUDA}. Se ha convertido en el método de
\emph{GPGPU} más popular actualmente en el ámbito del cálculo científico.
Es más fácil programar en \emph{CUDA}: en un estudio sobre implementaciones
de frameworks en \emph{CUDA} frente \emph{OpenCL} se encontró que \emph{OpenCL}
requiere el doble de lineas de código (en la
framework de Rodinia
\cite{memeti_benchmarking_2017,university_of_virginia_rodinia_nodate} )

En la mayoría de casos \emph{CUDA} ofrece mejores resultados que \emph{OpenCL} o
\emph{OpenACC} ya que la implementación de \emph{Nvidia} de \emph{CUDA} en sus
tarjetas esta mejor optimizada que \emph{OpenCL}.

\paragraph{OpenCL} es un estándar de código abierto al que se acogen la mayoría
de tarjetas gráficas actuales (Incluidas las de \emph{Nvidia}).
\cite{noauthor_opencl_2013}

En términos de funcionalidad y rendimiento es equiparable a \emph{CUDA}. La
ventaja de \emph{OpenCL} es que es compatible con la mayoría de tarjetas
gráficas. Es posible que si se tiene una tarjeta gráfica de \emph{Nvidia}
se obtuviera mejor rendimiento con \emph{CUDA}, pero la diferencia
probablemente es negligible (aunque depende del caso en concreto).

La otra desventaja ya se ha comentado antes, \emph{OpenCL} suele ser más
complejo para el programador que \emph{CUDA}.


\paragraph{OpenACC} es otro estándar de código abierto. A diferencia de
\emph{OpenCL}, \emph{OpenACC} usa directivas de compilador (\texttt{\#pragma})
similares a las de \emph{OpenMP} (para multithreading en \emph{CPU}). Estas
directivas tienen la ventaja que no afectan al código original y es más fácil
diferenciar el código original del \emph{GPGPU}.
\cite{noauthor_about_nodate}

\emph{OpenACC} es un lenguaje con un nivel abstracción más alto que \emph{CUDA}
y \emph{OpenCL}. Esto ofrece la ventaja de que el código es mucho mas portable
entre distintas arquitecturas pero restringe el nivel en que se puede optimizar
el código. Muchas optimizaciones a nivel del hardware especifico no se pueden
realizar con \emph{OpenACC} o es muy complicado.

\pagebreak
\subsection*{Interoperación}

\emph{OpenACC} es capaz de interoperar con \emph{CUDA}
sin necesidad de hacer muchos cambios en el código \cite{noauthor_3_2014}.

En el caso de \emph{OpenCL} no existe interoperabilidad con \emph{CUDA} ni con
\emph{OpenACC} de manera nativa. Es posible que se puedan usar los mismos
punteros de memoria para compartir datos entre \emph{OpenCL} y \emph{CUDA} o
\emph{OpenACC} pero no está garantizado.
Una posible opción usar un buffer de \emph{OpenGL} para guardar transferir la
información de uno a otro: si se \emph{bindean} los objetos en el mismo bufer de
\emph{OpenGL} ocuparan la misma posición de memoria en la \emph{GPU} y se podrán
usar en los dos.

La ventaja de la interoperabilidad entre los estándares es la posibilidad de usar
librerías escritas en otro estándar en el desarrollo de una aplicación. No es
necesario implementar funcionalidad que ya esta correctamente implementada y
testeada en una librería popular con otro lenguaje. Esto permite también que los
programadores se puedan especializar en un estándar en concreto y no tengan que
aprender distintos estándares para desarrollar programas en \emph{GPGPU}.

En el caso de \emph{OpenACC} la propia guia de programación nos habla de los
beneficios y desventajas de usar \emph{OpenACC} sugiriendo usar \emph{OpenACC}
en combinación con \emph{CUDA} o \emph{OpenCL}. Según la guía se recomienda usar
\emph{OpenACC} en la mayoría del código ya que su abstracción facilita la
programación y en partes criticas del programa recurrir a \emph{CUDA} o
\emph{OpenCL} si estos permiten mejores optimizaciones
\cite{noauthor_openacc_nodate}. Este enfoque es similar al que se usa en algunas
librerías de \emph{Python} donde gran parte del código esta escrito en \emph{Python}
pero las partes mas costosas se implementan directamente en \emph{C}.

\end{pregunta}
\begin{pregunta}{Hablando de texturas, ¿qué filtros existen?, ¿puedes describirlos? ¿qué
implicaciones tienen en el diseño de la GPU?} \sep{}

Hay diversos filtros: lineal, bi-lineal, tri-lineal anisotropico

\cite{heckbert_survey_1986}

\end{pregunta}
